{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input_game.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the database with respect to players\n",
    "\n",
    "We want to process the strategy of each player, so we have created a csv file corresponding to each player which contains all the turns in which that particular player is participating across all games. These csv files will be used later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach is that in all the rows we have the data of the last four turns and we have to predict the output of the 5th turn. This concept is used in training and testing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'game_id' column\n",
    "grouped_data = df.groupby('game_id')\n",
    "\n",
    "# Iterate over the groups\n",
    "for game_id, group_data in grouped_data:\n",
    "    print('Game ID:', game_id)\n",
    "    \n",
    "    # Create a new dataframe with the group\n",
    "    game_df = pd.DataFrame(group_data)\n",
    "    player1_id, player2_id = game_df['p1_id'].iloc[0], game_df['p2_id'].iloc[0]\n",
    "    \n",
    "    # Initialize new dataframes for each player's actions\n",
    "    player1_actions_df, player2_actions_df = pd.DataFrame(columns=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'X1', 'X2', 'X3', 'X4', 'y']), pd.DataFrame(columns=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'X1', 'X2', 'X3', 'X4', 'y'])\n",
    "    \n",
    "    # Iterate over game_df, considering the last 5 rows\n",
    "    for i in range(4, len(game_df)):\n",
    "        # Create a row with the previous 4 values of 'p1_action' and 'p2_action' and append it to the new dataframes\n",
    "        action_row = game_df.iloc[i-4:i+1].replace('CHEAT', -1).replace('TRUST', 1)\n",
    "        \n",
    "        # For player 1\n",
    "        player1_actions_df = pd.concat([player1_actions_df, pd.DataFrame({'x1': action_row.iloc[0]['p1_action'], 'x2': action_row.iloc[0]['p2_action'], 'x3': action_row.iloc[1]['p1_action'], 'x4': action_row.iloc[1]['p2_action'], 'x5': action_row.iloc[2]['p1_action'], 'x6': action_row.iloc[2]['p2_action'], 'x7': action_row.iloc[3]['p1_action'], 'x8': action_row.iloc[3]['p2_action'], 'y': action_row.iloc[4]['p1_action']}, index=[0])])\n",
    "        \n",
    "        # For player 2\n",
    "        player2_actions_df = pd.concat([player2_actions_df, pd.DataFrame({'x1': action_row.iloc[0]['p2_action'], 'x2': action_row.iloc[0]['p1_action'], 'x3': action_row.iloc[1]['p2_action'], 'x4': action_row.iloc[1]['p1_action'], 'x5': action_row.iloc[2]['p2_action'], 'x6': action_row.iloc[2]['p1_action'], 'x7': action_row.iloc[3]['p2_action'], 'x8': action_row.iloc[3]['p1_action'], 'y': action_row.iloc[4]['p2_action']}, index=[0])])\n",
    "\n",
    "    # Write dataframes to CSV files, append if files already exist\n",
    "    player1_actions_df.to_csv(str(player1_id) + '.csv', mode='a', header=False)\n",
    "    player2_actions_df.to_csv(str(player2_id) + '.csv', mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(8,1)\n",
    "        #self.fc2 = nn.Linear(4,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over a range of file indices\n",
    "for file_index in range(101, 202):\n",
    "    # Read data from a CSV file\n",
    "    filename = f'{file_index}.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    data_array = df.to_numpy()\n",
    "    \n",
    "    # Extract features and labels\n",
    "    features = data_array[:, 1:9]\n",
    "    labels = data_array[:, -1]\n",
    "    labels = (labels + 1) / 2\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    X = torch.tensor(features).float()\n",
    "    Y = torch.tensor(labels).float()\n",
    "    \n",
    "    # Set hyperparameters for training\n",
    "    learning_rate = 0.01\n",
    "    epochs = 2000\n",
    "    model = Net()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, Y)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if epoch % 100 == 0:\n",
    "        #     print(\"epoch: \", epoch, \" loss: \", loss.item())\n",
    "\n",
    "    # Extract parameters from the trained model\n",
    "    parameters_list = []\n",
    "    for param_tensor in model.state_dict():\n",
    "        param_array = model.state_dict()[param_tensor].numpy().flatten()\n",
    "        parameters_list.extend(param_array)\n",
    "\n",
    "    # Convert parameters to a NumPy array and print\n",
    "    parameters_array = np.array(parameters_list)\n",
    "    for param in parameters_array:\n",
    "        print(param, end=',')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 9)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('weights.csv', header = None)\n",
    "df = df.iloc[:, :-1]\n",
    "data = df.values\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the inertia and silhoutte score corresponding to each value of k (the number of clusters)\n",
    "The k-value with the maximum silhoutte score is the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(2, 21)\n",
    "\n",
    "# Initialize lists to store inertia and silhouette scores\n",
    "inertia_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Calculate inertia and silhouette score for each value of k\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(data)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(data, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haayu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids for k=18:\n",
      "[[ 3.010e-01  4.900e-02  6.200e-02  1.300e-02  6.880e-01 -1.570e-01\n",
      "   3.300e-02  2.290e-01  1.630e-01]\n",
      " [-7.450e-01 -6.940e-01 -5.880e-01 -5.490e-01 -5.270e-01 -8.380e-01\n",
      "  -4.040e-01 -1.173e+00 -6.220e-01]\n",
      " [ 2.040e-01  1.900e-02  1.400e-02  1.180e-01  1.900e-02  2.050e-01\n",
      "  -2.000e-03  2.196e+00  2.500e-02]\n",
      " [ 1.657e+00 -1.000e-03 -4.040e-01  8.500e-02 -4.150e-01 -2.800e-02\n",
      "  -4.210e-01 -1.000e-02  1.061e+00]\n",
      " [ 6.070e-01  1.650e-01  5.650e-01  2.020e-01  5.320e-01  2.370e-01\n",
      "   8.080e-01  3.270e-01 -4.900e-01]\n",
      " [ 2.530e-01  3.050e-01  4.900e-01  2.760e-01  3.040e-01  3.080e-01\n",
      "   5.380e-01 -2.059e+00  1.520e-01]\n",
      " [ 1.670e-01 -3.000e-02 -1.112e+00  3.500e-02 -4.100e-02 -1.000e-03\n",
      "   1.293e+00 -3.300e-02 -2.300e-01]\n",
      " [ 3.530e-01 -1.800e-02  1.820e-01  8.900e-02  6.390e-01 -3.920e-01\n",
      "  -8.490e-01  1.517e+00  9.630e-01]\n",
      " [ 1.260e-01 -1.700e-02  3.090e-01 -1.000e-03  6.810e-01  1.500e-02\n",
      "   1.598e+00  4.000e-03  3.800e-02]\n",
      " [ 1.120e-01  1.490e-01  1.900e-01  1.380e-01  3.100e-01 -1.800e-02\n",
      "   7.020e-01 -5.430e-01 -2.320e-01]\n",
      " [ 2.860e-01 -9.400e-02  1.480e-01  7.000e-03 -3.800e-02  1.044e+00\n",
      "   4.510e-01  1.207e+00  6.400e-01]\n",
      " [ 6.480e-01  1.280e-01  7.250e-01  8.600e-02  7.500e-01  1.020e-01\n",
      "   8.440e-01  1.250e-01  9.700e-02]\n",
      " [ 5.840e-01  2.900e-02  5.470e-01  4.200e-02  5.930e-01 -1.200e-02\n",
      "   5.400e-01  3.000e-03  8.700e-01]\n",
      " [-2.000e-03  1.700e-02  1.600e-02 -8.000e-03 -7.000e-03 -1.100e-02\n",
      "  -2.100e-02 -4.000e-03  3.000e-03]\n",
      " [ 6.190e-01 -4.700e-02  4.950e-01 -3.100e-02  6.140e-01  1.020e-01\n",
      "   5.480e-01  3.200e-02 -8.600e-01]\n",
      " [ 4.220e-01 -2.300e-02  2.330e-01  7.000e-02  7.030e-01  4.600e-02\n",
      "  -1.370e-01  3.100e-02  9.700e-02]\n",
      " [ 4.790e-01 -6.400e-02  6.260e-01  2.400e-02  5.390e-01  6.700e-02\n",
      "   7.410e-01  2.080e-01 -7.870e-01]\n",
      " [ 3.590e-01  1.420e-01  3.220e-01  3.230e-01  4.650e-01  3.890e-01\n",
      "   4.280e-01 -2.006e+00  1.390e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Set the number of clusters observed from the graph\n",
    "k = 18\n",
    "\n",
    "# Instantiate and fit KMeans model\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Obtain the cluster centers (means)\n",
    "means = kmeans.cluster_centers_\n",
    "\n",
    "# Print the means (centroids) for k=18\n",
    "print(\"Centroids for k=18:\")\n",
    "print(np.round(means, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haayu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: [1, 5, 7, 19, 27, 39, 76, 87, 125, 143, 162, 183]\n",
      "Cluster 1: [2, 3, 20, 118, 126, 138, 149, 150, 190]\n",
      "Cluster 2: [4, 14, 22, 23, 38, 64, 85, 96, 97, 105, 129, 148, 158, 167, 197]\n",
      "Cluster 3: [6, 8, 31, 43, 61, 99, 127, 133, 139, 186, 191, 196]\n",
      "Cluster 4: [9, 13, 60, 95, 104, 121, 128, 137, 175, 178, 181, 192, 193, 194]\n",
      "Cluster 5: [10, 11, 28, 41, 44, 75, 106, 112, 130, 135, 153, 163, 164, 198]\n",
      "Cluster 6: [12, 45, 69, 74, 80, 86, 100, 103, 154, 161, 171, 176, 199]\n",
      "Cluster 7: [15, 25, 33, 57, 108, 110, 111, 115, 136, 155, 172]\n",
      "Cluster 8: [16, 18, 81, 122, 160]\n",
      "Cluster 9: [17, 35, 46, 47, 51, 52, 63, 70, 82, 107, 134, 165, 177, 195, 201]\n",
      "Cluster 10: [21, 29, 59, 62, 84, 91, 109, 117, 132, 140]\n",
      "Cluster 11: [24, 32, 50, 68, 71, 83, 114, 119, 124, 156, 173, 187]\n",
      "Cluster 12: [26, 40, 48, 49, 54, 66, 90, 98, 102, 120, 123, 147, 151, 170, 184]\n",
      "Cluster 13: [30, 42, 67, 77, 79, 93, 113, 159, 174, 179, 180]\n",
      "Cluster 14: [34, 56, 58, 88, 144, 152, 157, 182]\n",
      "Cluster 15: [36, 37, 53, 55, 65, 131, 141, 146, 189, 200]\n",
      "Cluster 16: [72, 78, 94, 116, 142, 145, 166]\n",
      "Cluster 17: [73, 89, 92, 168, 169, 185, 188]\n"
     ]
    }
   ],
   "source": [
    "# Define the number of clusters\n",
    "k = 18\n",
    "\n",
    "# Instantiate and fit KMeans model\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Retrieve the cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Initialize a dictionary to store row numbers for each cluster\n",
    "cluster_rows = {i: [] for i in range(k)}\n",
    "\n",
    "# Assign each data point to its corresponding cluster\n",
    "for i, label in enumerate(labels):\n",
    "    # Adjust row numbering for consistency\n",
    "    row_number = i + 1 if i < 100 else i + 2\n",
    "    cluster_rows[label].append(row_number)\n",
    "\n",
    "# Sort the cluster rows\n",
    "sorted_clusters = sorted(cluster_rows.values())\n",
    "\n",
    "# Print the sorted cluster rows\n",
    "for cluster, rows in enumerate(sorted_clusters):\n",
    "    print(f\"Cluster {cluster}:\", rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
